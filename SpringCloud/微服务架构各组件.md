# 服务治理

## 什么是服务治理

服务治理是微服务架构中最核心最基本的模块。用于实现各个微服务的**自动化注册与发现**【实际上就是各种微服务ip/端口的统一管理(CRUD)】

![image-20221013105227696](https://image.itbaima.net/images/253/image-20231002103854935.png)

**服务注册**：在服务治理框架中，都会构建一个**注册中心**(管理ip/端口的地方)，每个服务单元向注册中心登记自己提供服务的详细信息。并在注册中心形成一张服务的清单(**建立服务名与ip/端口映射**)，服务注册中心需要以心跳的方式去监测清单中的服务是否可用，如果不可用，需要在服务清单中剔除不可用的服务。

![image-20221013110453454](images/image-20221013110453454.png)

**服务发现**：微服务去咨询注册中心，并获取所有服务清单，实现对具体服务的访问。

![image-20201029084800199](https://image.itbaima.net/images/253/image-20231002102867767.png)

综上，在微服务架构里，服务中心主要起到了**协调者**的作用：
1. 服务发现：
   **服务注册**：保存服务提供者和服务调用者的信息
   **服务订阅**：服务调用者订阅服务提供者的信息，注册中心向订阅者推送提供者的信息（**拉服务清单**）
2. 服务健康检测

   检测服务提供者的健康情况，如果发现异常，执行服务剔除

## 常见注册中心

**Zookeeper**
Zookeeper是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。

**Eureka**
Eureka是Springcloud Netflflix中的重要组件，主要作用就是做服务注册和发现。但是现在已经闭源

**Consul**
Consul是基于GO语言开发的开源工具，主要面向分布式，服务化的系统提供服务注册、服务发现和配置管理的功能。
Consul的功能都很实用，其中包括：服务注册/发现、健康检查、Key/Value存储、多数据中心和分布式一致性保证等特性。
Consul本身只是一个二进制的可执行文件，所以安装和部署都非常简单，只需要从官网下载后，在执行对应的启动脚本即可。

**Nacos**
Nacos是一个更易于构建云原生应用的动态服务发现、配置管理和服务管理平台。它是 SpringCloud Alibaba 组件之一，负责服务注册发现和服务配置。

![20190916151417612.png](https://image.itbaima.net/images/253/image-20231002109134814.png)

# 远程调用负载均衡

## 背景铺垫

微服务项目有一个重要的功能：可以很容易**动态扩缩容**

这里先不触及容器技术(docker/k8s这些)，传统的扩缩容简单理解就是服务集群。

**扩**：特定时期(比如促销，天灾人祸)一个微服务可能容易挂掉(撑不住/宕机)，那么多开几个就行
**缩**：特定时期过后，多开的微服务可以适当关掉多余的

***

以订单与商品服务为例子，分析一下扩
>如果某天公司要促销，单一的订单服务，商品服务不一定能撑得住，该怎么扩容。

扩容常用的手段就是集群

集群存在一个很大问题，客户端发起的请求让哪个微服务处理？解决方案：**负载均衡服务器**

## 负载均衡

通俗的讲，负载均衡就是将负载（工作任务，访问请求）分摊到多个操作单元（服务器,组件）上进行执行。

根据负载均衡发生位置的不同,一般分为**服务端负载均衡**和**客户端负载均衡**。

![image-20201029102235075](https://image.itbaima.net/images/253/image-20231002101780921.png)

**服务端负载均衡**
服务端负载均衡发生在服务提供者一方，比如常见的Nginx负载均衡

![image-20221013161421850](https://image.itbaima.net/images/253/image-20231002114930373.png)

**客户端负载均衡**
客户端负载均衡发生在服务请求的一方，也就是在发送请求之前已经选好了由哪个实例处理请求

![image-20221013164501474](https://image.itbaima.net/images/253/image-20231002111520915.png)

微服务调用关系中一般会选择**客户端负载均衡**，也就是在服务调用的一方来决定服务由哪个提供者执行。

# 服务熔断降级

## 高并发带来的问题

在微服务架构中，将业务拆分成一个个的服务，服务与服务之间可以相互调用，但是由于网络原因或者自身的原因，服务并不能保证服务的100%可用，如果单个服务出现问题，调用这个服务就会出现网络延迟，此时若有大量的网络涌入，会形成任务堆积，最终导致服务瘫痪。

## 服务器雪崩效应

前面同一个服务器不同接口间，当一个接口高频访问耗费完资源会影响到其他接口正常使用，如果将这个场景扩展到不同微服务间会怎么样呢？答案：**服务器雪崩**。

在分布式系统中,由于网络原因或自身的原因，服务一般无法保证 100% 可用。如果一个服务出现了问题，调用这个服务就会出现线程阻塞的情况，此时若有大量的请求涌入，就会出现多条线程阻塞等待，进而导致服务瘫痪。

由于服务与服务之间的依赖性，故障会传播，会对整个微服务系统造成灾难性的严重后果，这就是服务故障的 “**雪崩效应**” 

服务器的雪崩效应其实就是由于某个微小的服务挂了，导致整一大片的服务都不可用。类似生活中的雪崩效应,由于落下的最后一片雪花引发了雪崩的情况.

这种问题实际上是不可避免的，由于多种因素，比如网络卡顿、系统故障、硬件问题等，都存在一定可能，会导致这种极端的情况发生。因此，需要寻找一个应对这种极端情况的解决方案。
  
## 常见容错方案

**常见的容错思路**

**隔离机制**
比如服务A内限制有100个线程,，现在服务A可能会调用服务B，服务C，服务D。在服务A进行远程调用的时候，给不同的服务分配固定的线程，不会把所有线程都分配给某个微服务。比如调用服务B分配30个线程，调用服务C分配30个线程，调用服务D分配40个线程
这样进行资源的隔离，保证即使下游某个服务挂了，也不至于把服务A的线程消耗完。比如服务B挂了，这时候最多只会占用服务A的30个线程，服务A还有70个线程可以调用服务C和服务D

![image-20201029142100450](https://image.itbaima.net/images/253/image-20231002118048022.png)

**超时机制**
在上游服务调用下游服务的时候，设置一个最大响应时间，如果超过这个时间，下游未作出反应，就断开请求，释放掉线程。

![image-20201029143237828](https://image.itbaima.net/images/253/image-20231002114960203.png)

**限流机制**
限流就是限制系统的输入和输出流量，已达到保护系统的目的。
为了保证系统的稳固运行，一旦达到需要限制的阈值，就需要限制流量并采取少量措施以完成限制流量的目的。

![image-20201029143206491](https://image.itbaima.net/images/253/image-20231002118956105.png)

**熔断机制**
熔断机制是应对雪崩效应的一种微服务链路保护机制，当检测出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回”错误”的响应信息。当检测到该节点微服务响应正常后恢复调用链路。
总而言之，当下游服务因访问压力过大而响应变慢或失败，上游服务为了保护系统整体的可用性，可以暂时切断对下游服务的调用。这种牺牲局部，保全整体的措施就叫做熔断。

![image-20201029143128555](https://image.itbaima.net/images/253/image-20231002105985488.png)

服务熔断一般有三种状态：
- 熔断关闭状态（Closed）
  服务没有故障时，熔断器所处的状态，对调用方的调用不做任何限制
- 熔断开启状态（Open）
  后续对该服务接口的调用不再经过网络，直接执行本地的fallback方法
- 半熔断状态（Half-Open）
  尝试恢复服务调用，允许有限的流量调用该服务，并监控调用成功率。如果成功率达到预期，则说明服务已恢复，进入熔断关闭状态；如果成功率仍旧很低，则重新进入熔断开启状态。

实际上，熔断就是在降级的基础上进一步升级形成的，在**一段时间内多次调用失败，那么就直接升级为熔断**。

![image-20230306230247024](https://s2.loli.net/2023/03/06/3oiHLFWO9jVpswK.png)

**降级机制**
服务降级是一个兜底方案，比较温柔，虽然服务本身不可用，但并不会直接返回错误，而是可以提供一个补救措施，保证正常响应数据给请求者。这样相当于服务依然可用，只是服务能力下降了。

![image-20201029143456888](https://image.itbaima.net/images/253/image-20231002105630359.png)

## 常见的容错组件

**Hystrix**
Hystrix是由Netflix开源的一个延迟和容错库，用于隔离访问远程系统、服务或者第三方库，防止级联失败，从而提升系统的可用性与容错性。

**Resilience4J**
Resilicence4J一款非常轻量、简单，并且文档非常清晰、丰富的熔断工具，这也是Hystrix官方推荐的替代产品。不仅如此，Resilicence4j原生支持Spring Boot 1.x/2.x，而且监控也支持和prometheus等多款主流产品进行整合。

**Sentinel**
Sentinel 是阿里巴巴开源的一款断路器实现，本身在阿里内部已经被大规模采用，非常稳定。

# 服务网关

## 什么是API网关

在微服务架构中，一个系统会被拆分为很多个微服务。
如果没有网关的存在，我们只能在客户端记录每个微服务的地址，然后分别去调用。

![image-20201030144013742](https://image.itbaima.net/images/253/image-20231002109324172.png)

这样的架构，会存在着诸多的问题：
- 客户端多次请求不同的微服务，增加客户端代码或配置编写的复杂性
- 认证复杂，每个服务都需要独立认证。
- 微服务做集群的情况下，客户端并没有负责均衡的功能

上面的这些问题可以借助**API网关**来解决。

API网关就是**系统的统一入口**，它封装了应用程序的内部结构，为客户端提供统一服务，一些与业务本身功能无关的公共逻辑可以在这里实现，诸如认证、鉴权、监控、路由转发等等。

添加上API网关之后，系统的架构图变成了如下所示：

![image-20201030144534269](https://image.itbaima.net/images/253/image-20231002108684133.png)

网关是如何知道微服务的地址?网关如何进行负载均衡呢？
>网关需要将自己的信息注册到注册中心上并且拉取其他微服务的信息，然后在调用的时候基于一些负载均衡的组件实现负载均衡。

网关的作用：
**1>请求分发  2>负载均衡  3>过滤拦截   4>网络隔离**

## 常见网关介绍

**Nginx+lua**
使用nginx的反向代理和负载均衡可实现对api服务器的负载均衡及高可用。lua是一种脚本语言，可以来编写一些简单的逻辑，nginx支持lua脚本

**Kong**
基于Nginx+Lua开发，性能高，稳定，有多个可用的插件(限流、鉴权等等)可以开箱即用。 
问题：只支持Http协议；二次开发，自由扩展困难；提供管理API，缺乏更易用的管控、配置方式。

**Zuul** 
Netflix开源的网关，功能丰富，使用JAVA开发，易于二次开发
问题：缺乏管控，无法动态配置；依赖组件较多；处理Http请求依赖的是Web容器，性能不如Nginx，Spring Cloud Gateway

**Gateway**
Spring公司为了替换Zuul而开发的网关服务
SpringCloud alibaba技术栈中并没有提供自己的网关，可以采用Spring Cloud Gateway来做网关

# 链路追踪

由于服务单元数量众多，业务的复杂性，如果出现了错误和异常，很难去定位。
主要体现在，一个请求可能需要调用很多个服务，而内部服务的调用复杂性，决定了问题难以定位。

所以微服务架构中，必须实现分布式链路追踪，去跟进一个请求到底有哪些服务参与，参与的顺序是怎样的，从而达到每个请求的步骤清晰可见，出了问题，很快定位。

分布式链路追踪（Distributed Tracing），就是将一次分布式请求还原成调用链路，进行日志记录，性能监控，并将一次分布式请求的调用情况集中展示。比如各个服务节点上的耗时、请求具体到达哪台机器上、每个服务节点的请求状态等等。

## 常见的链路追踪技术

**cat**
由大众点评开源，基于Java开发的实时应用监控平台，包括实时应用监控，业务监控。 
集成方案是通过代码埋点的方式来实现监控，比如：拦截器，过滤器等。对代码的侵入性很大，集成成本较高。风险较大。

**zipkin**
由Twitter公司开源，开放源代码分布式的跟踪系统，用于收集服务的定时数据，以解决微服务架构中的延迟问题，包括：数据的收集、存储、查找和展现。
该产品结合spring-cloud-sleuth使用较为简单， 集成很方便，但是功能较简单。

**pinpoint**
Pinpoint是韩国人开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件，UI功能强大，接入端无代码侵入。

**skywalking**
SkyWalking是本土开源的基于字节码注入的调用链分析，以及应用监控分析工具。特点是支持多种插件，UI功能较强，接入端无代码侵入。目前已加入Apache孵化器。

**Sleuth**
SpringCloud 提供的分布式系统中链路追踪解决方案。

# 配置中心

## 服务配置中心介绍

微服务架构下配置文件的一些问题：

1. 配置文件相对分散。在一个微服务架构下，配置文件会随着微服务的增多变的越来越多，而且分散在各个微服务中，不好统一配置和管理。
2. 配置文件无法区分环境。微服务项目可能会有多个环境，例如：测试环境、预发布环境、生产环境。每一个环境所使用的配置理论上都是不同的，一旦需要修改，就需要去各个微服务下手动维护，这比较困难。
3. 配置文件无法实时更新。修改了配置文件之后，必须重新启动微服务才能使配置生效，这对一个正在运行的项目来说是非常不友好的。

基于上面这些问题，就需要引入**配置中心**

**配置中心的思路**
首先把项目中各种配置全部都放到一个集中的地方进行统一管理，并提供一套标准的接口。
当各个服务需要获取配置的时候，就来配置中心的接口拉取自己的配置。
当配置中心中的各种参数有更新的时候，也能通知到各个服务实时的过来同步最新的信息，使之动态更新。

加入了服务配置中心之后，系统架构图会变成下面这样：

![image-20201101222022206](https://image.itbaima.net/images/253/image-20231002108477705.png)

## 常见的服务配置中心

**Apollo**
Apollo是由携程开源的分布式配置中心。特点有很多，比如：配置更新之后可以实时生效，支持灰度发布功能，并且能对所有的配置进行版本管理、操作审计等功能，提供开放平台API。并且资料也写的很详细。

**Disconf**
Disconf是由百度开源的分布式配置中心。它是基于Zookeeper来实现配置变更后实时通知和生效的。

**SpringCloud Config**
这是Spring Cloud中带的配置中心组件。它和Spring无缝集成，使用起来非常方便，并且它的配置存储支持Git。
不过它没有可视化的操作界面，配置的生效也不是实时的，需要重启或去刷新。

**Nacos**
这是SpingCloud alibaba技术栈中的一个组件，除了做服务注册中心之外，其实它还集成了服务配置的功能，可以直接使用它作为服务配置中心。

# 微服务CAP原则

![image-20230306230815241](https://s2.loli.net/2023/03/06/9k6oeMZIE28T3t7.png)

> CAP原则又称CAP定理，指的是在一个分布式系统中，存在Consistency（一致性）、Availability（可用性）、Partition tolerance（分区容错性），三者不可同时保证，最多只能保证其中的两者。   
>
> 一致性（C）：在分布式系统中的所有数据备份，在同一时刻都是同样的值（所有的节点无论何时访问都能拿到最新的值）
>
> 可用性（A）：系统中非故障节点收到的每个请求都必须得到响应（比如服务降级和熔断，其实就是一种维持可用性的措施，虽然服务返回的是没有什么意义的数据，但是不至于用户的请求会被服务器忽略）
>
> 分区容错性（P）：一个分布式系统里面，节点之间组成的网络本来应该是连通的，然而可能因为一些故障（比如网络丢包等），使得有些节点之间不连通了，整个网络就分成了几块区域，数据就散布在了这些不连通的区域中（这样就可能出现某些被分区节点存放的数据访问失败，我们需要来容忍这些不可靠的情况）

总的来说，数据存放的节点数越多，分区容忍性就越高，但是要复制更新的次数就越多，一致性就越难保证。同时为了保证一致性，更新所有节点数据所需要的时间就越长，那么可用性就会降低。

所以说，只能存在以下三种方案：

## AC

>可用性+一致性

要同时保证可用性和一致性，代表着某个节点数据更新之后，需要立即将结果通知给其他节点，并且要尽可能的快，这样才能及时响应保证可用性，这就对网络的稳定性要求非常高。
但是实际情况下，网络很容易出现丢包等情况，并不是一个可靠的传输，如果需要避免这种问题，就只能将节点全部放在一起，但是这显然违背了分布式系统的概念，所以对于分布式系统来说，很难接受。

## CP

>一致性+分区容错性

为了保证一致性，那么就得将某个节点的最新数据发送给其他节点，并且需要等到所有节点都得到数据才能进行响应，同时有了分区容错性，那么代表我们可以容忍网络的不可靠问题，所以就算网络出现卡顿，那么也必须等待所有节点完成数据同步，才能进行响应，因此就会导致服务在一段时间内完全失效，所以可用性是无法得到保证的。

## AP

>可用性+分区容错性

既然CP可能会导致一段时间内服务得不到任何响应，那么要保证可用性，就只能放弃节点之间数据的高度统一，也就是说可以在数据不统一的情况下，进行响应，因此就无法保证一致性了。
虽然这样会导致拿不到最新的数据，但是只要数据同步操作在后台继续运行，一定能够在某一时刻完成所有节点数据的同步，那么就能实现**最终一致性**，所以AP实际上是最能接受的一种方案。

比如Eureka集群，它使用的就是AP方案，Eureka各个节点都是平等的，少数节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。
而Eureka客户端在向某个Eureka服务端注册时如果发现连接失败，则会自动切换至其他节点。只要有一台Eureka服务器正常运行，那么就能保证服务可用（**A**），只不过查询到的信息可能不是最新的（**C**）